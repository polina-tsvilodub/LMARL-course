{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2\n",
    "\n",
    "This session will provide an introduction to Cognitive Architectures (CA), a computational functional description and a theory of human-like minds.\n",
    "\n",
    "Slides from the second lecture can be found [here](https://github.com/polina-tsvilodub/LMARL-course/tree/main/lmarl-course/slides/session2.pdf).\n",
    "\n",
    "The discussion of the day will be on the paper by [Laird, Lebiere & Rosenbloom (2017). A Standard Model of the Mind: Toward a Common Computational Framework Across Artificial Intelligence, Cognitive Science, Neuroscience, and Robotics](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2744)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Below are some questions to recap and apply central concepts of the second session. Please try answer them for yourself. \n",
    "Further, exercises to familiarize yourself with theoretically-informed approaches of cognitive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.1.: What are cognitive architectures?</span></strong>\n",
    "> \n",
    "> 1. What are the advantages and limitations of structured representations and a structured architecture? Name and briefly explain at least two advantages and disadvantages for each.\n",
    "> \n",
    "> 2. What is the conceptual difference between a cognitive architecture and the BDI model?\n",
    ">\n",
    "> 3. Consider the core modules of cognitive architectures: different types of memory, perception, motor modules, attention, action selection, learning. Which of these can one map onto aspects of LLMs? Map and explain briefly.\n",
    ">\n",
    "> 4. Compare the cognitive cycle described in the standard model and the architecture of Generative Agents. Which components match, which are missing or differ?\n",
    ">\n",
    "> 5. Do you consider the Generative Agents architecture a better simulator of human behavior that an LLM only? Name and briefly explain two arguments.\n",
    ">\n",
    "> 6. The Standard Model formalizes bounded rationality rather than fully optimal cognition. What would speak in favor, would would speak against having boundedly rational artificial agents, if they were a part of human every day life? Name one argument each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click below to see suggested solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "1. What are the advantages and limitations of structured representations and a structured architecture? Name and briefly explain at least two advantages and disadvantages for each.\n",
    "    1. structured representations: + relations can be defined + interpretable representation - inflexible for definition, limited expressiveness (difficult conversion from real world signals) - inflexible structure (not allowing probabilistic notions, gradedness)\n",
    "    2. structured architecture: + transparency + potential human-likeness - the bitter lesson - difficulty of building - task specificity\n",
    "\n",
    "2. What is the conceptual difference between a cognitive architecture and the BDI model?\n",
    "    1. CA is a functional formalization of the human mind (more abstract)\n",
    "    2. BDI is a concrete model; more of a property-based description\n",
    "\n",
    "3. Consider the core modules of cognitive architectures: different types of memory, perception, motor modules, attention, action selection, learning. Which of these can one map onto aspects of LLMs? Map and explain briefly.\n",
    "    1. long-term memory: might be compared to in-weight stored information (however, procedural, semantic etc are all represented in the same way); however, it cannot be updated without re-training\n",
    "    2. short-term memory / working memory: maybe in-context information\n",
    "    3. perception: maybe encoded information from user input, but arguably no default grouping / categorization happens\n",
    "    4. motor modules: none\n",
    "    5. attention: attention in transformers\n",
    "    6. action selection: predicting the next token (WTA / probabilistic selection)\n",
    "    7. learning — see memory\n",
    "\n",
    "4. Compare the cognitive cycle described in the standard model and the architecture of Generative Agents. Which components match, which are missing or differ?\n",
    "    1. cognitive cycle: \"... driven by procedural memory. In each cycle, procedural memory tests the contents of working memory and selects an action that modifies working memory. These modifications can lead to further actions retrieved from procedural memory, or they can initiate operations in other modules, such as motor action, memory retrieval, or perceptual acquisition, whose results will in turn be deposited back in working memory.\"\"\n",
    "    2. Generative agents: action selection based on memory, but rather “long-term memory contents\" in-weight; although implementation is arguably working-memory style where the currently constructed prompt is the working memory. However, action does not directly modify the working memory, but goes through the effect on environment, and loop of reflection, planning\n",
    "\n",
    "5. Do you consider the Generative Agents architecture a better simulator of human behavior that an LLM only? Name and briefly explain two arguments.\n",
    "    1. works more accurately on a behavioral level (better performance)\n",
    "    2. more structure known from humans → potentially more human-likeness, but it assumes that there is a causal connection between the structure of the agent and the actual behavioral outputs (rather than spurious correlations)\n",
    "    3. note: for pure behavioral simulation rather than *explanation* the functional architecture of the simulator might actually irrelevant\n",
    "\n",
    "6. The Standard Model formalizes bounded rationality rather than fully optimal cognition. What would speak in favor, would would speak against having boundedly rational artificial agents, if they were a part of human every day life? Name one argument each.\n",
    "    - + human-likeness → naturalness of interaction\n",
    "    - - we might want that the artificial systems are safer, better than humans on certain applications\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.2.: Aspects of cognitive architectures</span></strong>\n",
    ">\n",
    "> 1. One of the modules in cognitive architectures is perception, which interacts with memory. One core functionality of these subprocesses in human cogntion is categorization, i.e., assigning discrete categories to perceived stimuli, based on our knowledge (memory) of different categories. While ML models are highly accurate at categorization tasks, they need large amounts of training data that is implausible from a cognitive perspective. Please follow [this tutorial](https://alanjern.github.io/computational-psych-book/04-categorization.html) to see an example of a model from cognitive psychology where categorization happens based on a smaller number of examples.\n",
    "> \n",
    ">   a. What is the categorization \"criterion\" that is used? \n",
    ">   b. What do you think are strengths and limitations of this model?\n",
    ">\n",
    "> 2. Selection of relevant aspects to remember is one of the most difficult aspects of building models that reliably interact with the complex real world. Re-using the code and LLM from session 1, compare different approaches to scoring the relevance of different observations of the environment. Your task is to compare these approaches to the relevance scoring used in the Generative Agents architecture (see prompt on slides). The different approaches are roughly described, but you need to formulate the respective prompts. Feel free to play around with the set of observations, too. Do you see differences depedning on scoring? Do the results conform your intuitions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### environment set up: ####\n",
    "# set of observations\n",
    "observations = [\n",
    "    \"It's sunny outside.\",\n",
    "    \"You saw a big rock fall from the sky.\",\n",
    "    \"You saw a tiny dog in front of your house.\", \n",
    "    \"You met your best friend today.\",\n",
    "    \"You met your partner whom you haven't seen for a long time.\",\n",
    "    \"You found your high school diploma.\",\n",
    "    \"You ate cereal for lunch.\"\n",
    "]\n",
    "# current memory\n",
    "memory = [\n",
    "    \"You have a college diploma.\",\n",
    "    \"You live in a small town.\",\n",
    "    \"You have a cat.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: relevance of remembering is based on similarity of current memories and the observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2: relevance is based on importance of the observation for generalization, success in the future -- \n",
    "# e.g., could roughly approximated by expected number of situations which might rely on this knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 3: relevance scoring prompting from Generative agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 4 (more advanced): multi-step prompting: rate relevance following generative agents, and also unexpectedness of the observation\n",
    "# in two separate LM calls and then rate overall retention score based on the predicted scores "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
