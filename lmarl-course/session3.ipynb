{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3\n",
    "\n",
    "This session will provide an introduction to Reinforcement Learning, the field of study and set of methods for computational formalization of learning goal-directed behavior.\n",
    "\n",
    "Slides from the third lecture can be found [here](https://github.com/polina-tsvilodub/LMARL-course/tree/main/lmarl-course/slides/session3.pdf).\n",
    "\n",
    "The discussion of the day will be on the paper by [Anthony et al. (2017). Thinking Fast and Slow with Deep Learning and Tree Search](https://arxiv.org/abs/1705.08439).\n",
    "\n",
    "For those who is curious, one general textbook on Reinforcement Learning can be found [here](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 3.1.: Reinforcement Learning & Agency</span></strong>\n",
    "> \n",
    "> 1. What types of information would be difficult / inefficient to learn from trial-and-error learning?\n",
    "> \n",
    "> 2. Learning from experts constitutes and important aspect of human learning. What are some situations where humans learn from experts (implicitly / explicitly)? What makes learning from experts efficient?\n",
    ">\n",
    "> 3. The notion of world / environment models is prominent in, e.g., Bayesian approaches to cognitive science. We have seen an example of a Bayesian model in the context of category prediction, whoch built on an intuitive model of how categories are constructed. This model was used to infer likely category of an observation via Bayesian inference. What, if any, are the differences between this \"model of the world\" and how it's used to perform a task, the how the model of the world (or, environment) is used in model-based RL (e.g., ExIt)?\n",
    ">\n",
    "> 4. Try to come up with a reward formalizing the goal of successfully navigating from the classroom to, e.g., your home. Which aspects are more difficult to formalize, which are easier? How does this relate to considerations of AI safety?\n",
    ">\n",
    "> 5. What is the difference between reward misspecification and reward hacking? Provide a conceivable example of reward hacking in the context of LMs.\n",
    ">\n",
    "> 6. What steps are involved in a typical RLHF pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practical terms, the Gymnasium environment / package is one of the most practical tools for training RL models. Using the following tutorial, you will learn the basics of the environment with a basic task from RL, the multu-armed bandit task. In this task, the agent has to learn the rewards associated with a finite set of choice options (or, arms) through making repeated choices. \n",
    "As discussed in the lecture, you can think of repeatedly exploring restaurant options in a new town as an example of this task. Please read the following primer in order to solve the following exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 3.2.: Multi-armed bandit tasks in the Gym</span></strong>\n",
    ">\n",
    "> \n",
    "> 1. Please head to [this webbook](https://arena-chapter2-rl.streamlit.app/[2.1]_Intro_to_RL) and navigate to the exercise Google Colab that (can be found right at the top of the page). Please work through the first section (\"Multi-Armed Bandit\") of the exercise sheet, specifically, implementing the `RandomAgent`, and the `RewardAveraging`. The conceptual idea of averaging is described in detail [here, Sections 2.1--2.7](https://www.google.com/url?q=https%3A%2F%2Fwww.andrew.cmu.edu%2Fcourse%2F10-703%2Ftextbook%2FBartoSutton.pdf), and in a brief format as relevant for the task [here](https://polina-tsvilodub.github.io/RL4-language-model-training/homework1.html#exercise-3-20-points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
